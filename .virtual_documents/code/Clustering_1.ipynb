import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import seaborn as sns
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.stats import f_oneway, kruskal, pointbiserialr,chisquare,chi2_contingency
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression


data = pd.read_csv("../objects/df_imputed_not_complete_874.csv", index_col=0)
data.head()


data.drop('RID', axis = 1, inplace=True)


data.describe()


cat_features = [col for col in data.columns if (
    col.startswith("NX") or
    col.startswith("PX") or
    col.startswith("PT") or
    col.startswith("MH") or
    (col.startswith("GD") and col != "GDTOTAL") or
    col in ["mother", "father", "HMSCORE", "apoe",]
)]


cont_features = [col for col in data.columns if 
                 col not in cat_features and 
                col != "DIAGNOSIS"]


kr_stat = []
kr_p = []
for f in cont_features:
    groups = [data[f][data['DIAGNOSIS'] == cat] for cat in data['DIAGNOSIS'].unique()]
    kruskal_result = kruskal(*groups)
    kr_stat.append(kruskal_result.statistic)
    kr_p.append(kruskal_result.pvalue)
    


result_kw = pd.DataFrame({"feature": cont_features,
                         "stat": kr_stat,
                         "pvalue": kr_p})


result_kw[result_kw['pvalue'] < 0.01]


cont_features_sig = result_kw[result_kw['pvalue'] < 0.01].feature.tolist()


data_cor = data[cont_features_sig]
corr = data_cor.corr()
corr.style.background_gradient(cmap='coolwarm')


cr = []

for f in cat_features:
    cont_tab = pd.crosstab(index=data['DIAGNOSIS'], columns=data[f])
    chi2, p, dof, expected = chi2_contingency(cont_tab)
    n = cont_tab.sum().sum()
    phi2 = chi2 / n
    r, k = cont_tab.shape
    cramers_v = np.sqrt(phi2 / min(r-1, k-1))
    cr.append(cramers_v)
    #print(f"CramÃ©r's V: {cramers_v:.2f}")
    #print(chisquare(cont_tab, axis=None))


result_kramer = pd.DataFrame({"feature": cat_features,
                         "Kramer": cr})


result_kramer = result_kramer.sort_values(by="Kramer", ascending=False)


result_kramer[result_kramer['Kramer'] > 0.1]


cat_features_sig = result_kramer[result_kramer['Kramer'] > 0.1].feature.tolist()


feat = cont_features_sig + cat_features_sig


feat


#feat = ["PTEDUCAT", "VSPULSE", "NXGAIT", "LIMMTOTAL", "LDELTOTAL", "MHPSYCH", "GDBORED", "GDDROP", "GDMEMORY", "GDHOPE", "GDBETTER", "HMT15", "HMT16", "HMT8", "apoe"]


data[feat]


X = data[cont_features]  # Independent variables
y = data['DIAGNOSIS']               # Dependent variable (for example)
mi_scores = mutual_info_classif(X, y)


mi_df = pd.DataFrame({"mi":mi_scores,
                      "feature": cont_features})


mi_df = mi_df.sort_values(by="mi", ascending=False)
mi_df


mi_df[mi_df["mi"] > 0.04 ]


X = data[cat_features]  # Independent variables
y = data['DIAGNOSIS']               # Dependent variable (for example)
mi_scores = mutual_info_classif(X, y)


mi_df = pd.DataFrame({"mi":mi_scores,
                      "feature": cat_features})


mi_df = mi_df.sort_values(by="mi", ascending=False)
mi_df


mi_df[mi_df["mi"] > 0.04 ]


#feat_old = ['NXGAIT', "MHPSYCH", "GDDROP", "GDBORED", "GDMEMORY", "GDBETTER", "GDTOTAL", "HMT15", "HMT8","LDELTOTAL", "LIMMTOTAL"]


data_feat = data[feat]
outcome = data['DIAGNOSIS']


pipeline = Pipeline([("scaler", StandardScaler()), ("pca", PCA(n_components=2)),])


pca_data = pd.DataFrame(
    pipeline.fit_transform(data_feat),
    columns=["PC1", "PC2"],
    index=data_feat.index
)
pca_data['outcome'] = outcome
pca_step = pipeline.steps[1][1]


g = sns.scatterplot(data=pca_data, x="PC1", y="PC2", hue="outcome")


# Add variance explained by the
g.set_xlabel(f"PC1 ({pca_step.explained_variance_ratio_[0]*100:.2f} %)")
g.set_ylabel(f"PC2 ({pca_step.explained_variance_ratio_[1]*100:.2f} %)")

#plt.savefig("PCA_with_loadings.tiff", dpi=300)
plt.show()


pca_MCI = pca_data[pca_data["outcome"] == "MCI"]


gmm = GaussianMixture(n_components=3, covariance_type='tied')
labels = gmm.fit_predict(pca_MCI.iloc[:,0:2])


labels_MCI = np.where(labels == 0, "MCI1", np.where(labels == 1, "MCI2", "MCI3"))


pca_data["cluster"] = pca_data["outcome"]
pca_data.loc[pca_data["outcome"] == "MCI", "cluster"] = labels_MCI


g = sns.scatterplot(data=pca_data, x="PC1", y="PC2", hue="cluster")


# Add variance explained by the
g.set_xlabel(f"PC1 ({pca_step.explained_variance_ratio_[0]*100:.2f} %)")
g.set_ylabel(f"PC2 ({pca_step.explained_variance_ratio_[1]*100:.2f} %)")

#plt.savefig("PCA_with_loadings.png", dpi=200)
plt.show()


grouped_data = pca_data.groupby('cluster')[['PC1', 'PC2']].mean()
Z = linkage(grouped_data, method='ward')

# Create a dendrogram to visualize the clustering
plt.figure(figsize=(10, 7))
dendrogram(Z, labels=grouped_data.index)  # 'outcome' includes AZ, Control, MCI1, MCI2, MCI3
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()


data_feat_cluster = data_feat.copy()
data_feat_cluster["cluster"] = pca_data["cluster"]


data_feat_cluster = data_feat_cluster[data_feat_cluster["cluster"].isin(["MCI1","MCI2", "MCI3"])]


data_feat_cluster['cluster'] = data_feat_cluster['cluster'].replace({'MCI1': 'MCI_Middle', 'MCI2':'MCI_AD', 'MCI3': 'MCI_Healthy'})


data_feat_cluster.head()


cont_vars = ["HMT15", "LDELTOTAL", "LIMMTOTAL"]


for var in cont_vars:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x='cluster', y=var, data=data_feat_cluster)
    plt.title(f'Boxplot of {var} by MCI1, MCI2, MCI3')
    plt.show()

    # Perform statistical test (ANOVA or Kruskal-Wallis)
    mci1_data = data_feat_cluster[data_feat_cluster['cluster'] == 'MCI1'][var]
    mci2_data = data_feat_cluster[data_feat_cluster['cluster'] == 'MCI2'][var]
    mci3_data = data_feat_cluster[data_feat_cluster['cluster'] == 'MCI3'][var]

    # Choose test based on assumptions (ANOVA or Kruskal-Wallis)
    if data_feat_cluster[var].dtype == 'float64':  # Assuming normality for demonstration
        stat, p_value = f_oneway(mci1_data, mci2_data, mci3_data)
        print(f"ANOVA for {var}: F = {stat:.3f}, p = {p_value:.3f}")
    else:
        stat, p_value = kruskal(mci1_data, mci2_data, mci3_data)
        print(f"Kruskal-Wallis for {var}: H = {stat:.3f}, p = {p_value:.3f}")



