import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import metrics
from imblearn.over_sampling import SMOTE
from catboost import CatBoostClassifier
import funcs
from sklearn.metrics import roc_curve, auc, confusion_matrix


data = pd.read_csv("../objects/df_imputed_not_complete_874.csv", index_col=0)
data.head()


outcome = data['DIAGNOSIS']
outcome = outcome.replace({"Control": 0,
                  "MCI": 1,
                  "AZ": 2})


data.drop(['RID', 'DIAGNOSIS'], axis = 1, inplace=True)





cat_features = [col for col in data.columns if (
    col.startswith("NX") or
    col.startswith("PX") or
    col.startswith("PT") or
    col.startswith("MH") or
    (col.startswith("GD") and col != "GDTOTAL") or
    col in ["mother", "father", "HMSCORE", "apoe"]
)]


X, X_test, y, y_test = train_test_split(data, outcome, test_size=0.2, random_state=43)


smote_over = SMOTE(random_state=44)
X, y = smote_over.fit_resample(X, y)


best_params_baseline = {'iterations': 2000, 
                        'learning_rate': 0.21352331247419792, 
                        'l2_leaf_reg': 1.069229165770847, 
                        'bagging_temperature': 0.5393757099444308, 
                        'random_strength': 1.9024222669286768, 
                        'depth': 6, 
                        'min_data_in_leaf': 84,
                        'random_seed': 42,
                        'loss_function': "MultiClass",
                        'colsample_bylevel': 0.6168349577162479}


final_model = CatBoostClassifier(**best_params_baseline, verbose=False)


final_model.fit(X, y)


predictions = final_model.predict(X_test)
predictions_proba = final_model.predict_proba(X_test)


funcs.metrics_merged(y_test, predictions, predictions_proba)


confusion_matrix(y_test, predictions)


n_classes = len(np.unique(y_test))
class_labels = ["Control", "MCI", "AD"]
colors = ['blue', 'red', 'green']  # Define colors for the ROC curves

# Create a plot for ROC curves
plt.figure(figsize=(10, 8))

for i in range(n_classes):
    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test == i, predictions_proba[:, i])  # True binary labels for each class
    roc_auc = auc(fpr, tpr)  # Calculate AUC

    # Plot ROC curve
    plt.plot(fpr, tpr, color=colors[i], label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')

# Plotting the diagonal line
plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')  # Diagonal line for random guessing
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves for Multi-class')
plt.legend(loc='lower right')
plt.grid()
plt.show()


importances = final_model.feature_importances_
feature_names = X.columns  # If using a DataFrame, otherwise, use the appropriate feature list

# Sort feature importances
indices = np.argsort(importances)[::-1][:10]
# Create a bar plot of feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(10), importances[indices], align="center")
plt.xticks(range(10), feature_names[indices], rotation=90)
plt.xlim([-1, 10])
plt.ylabel("Importance")
plt.xlabel("Feature")
plt.tight_layout()
plt.show()
