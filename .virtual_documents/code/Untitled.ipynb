import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from scipy.stats import kruskal,chisquare,chi2_contingency
from imblearn.over_sampling import SMOTE
from catboost import CatBoostClassifier
import funcs
from sklearn.metrics import roc_curve, auc, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.mixture import GaussianMixture
from scipy.cluster.hierarchy import dendrogram, linkage


data = pd.read_csv("../objects/df_imputed_not_complete_874.csv", index_col=0)
data.head()


data.drop(['RID'], axis = 1, inplace=True)








cat_features = [col for col in data.columns if (
    col.startswith("NX") or
    col.startswith("PX") or
    col.startswith("PT") or
    col.startswith("MH") or
    (col.startswith("GD") and col != "GDTOTAL") or
    col in ["mother", "father", "HMSCORE", "apoe",]
)]


cont_features = [col for col in data.columns if 
                 col not in cat_features and 
                col != "DIAGNOSIS"]


kr_stat = []
kr_p = []
for f in cont_features:
    groups = [data[f][data['DIAGNOSIS'] == cat] for cat in data['DIAGNOSIS'].unique()]
    kruskal_result = kruskal(*groups)
    kr_stat.append(kruskal_result.statistic)
    kr_p.append(kruskal_result.pvalue)



result_kw = pd.DataFrame({"feature": cont_features,
                         "stat": kr_stat,
                         "pvalue": kr_p})


result_kw[result_kw['pvalue'] < 0.01]


cont_features_sig = result_kw[result_kw['pvalue'] < 0.01].feature.tolist()


cr = []

for f in cat_features:
    cont_tab = pd.crosstab(index=data['DIAGNOSIS'], columns=data[f])
    chi2, p, dof, expected = chi2_contingency(cont_tab)
    n = cont_tab.sum().sum()
    phi2 = chi2 / n
    r, k = cont_tab.shape
    cramers_v = np.sqrt(phi2 / min(r-1, k-1))
    cr.append(cramers_v)


result_kramer = pd.DataFrame({"feature": cat_features,
                         "Kramer": cr})
result_kramer = result_kramer.sort_values(by="Kramer", ascending=False)


result_kramer[result_kramer['Kramer'] > 0.1]


cat_features_sig = result_kramer[result_kramer['Kramer'] > 0.1].feature.tolist()


feat = cont_features_sig + cat_features_sig





X, X_test, y, y_test = train_test_split(data[feat], outcome, test_size=0.2, random_state=43)


smote_over = SMOTE(random_state=44)
X, y = smote_over.fit_resample(X, y)


best_params_feat_selection = {"iterations": 1800, 
                              "learning_rate": 0.20649746303659136, 
                              "l2_leaf_reg": 4.37841702433753, 
                              "bagging_temperature": 1.6766419657563723, 
                              "random_strength": 1.9555985333019168, 
                              "depth": 7, 
                              "min_data_in_leaf": 91, 
                              'random_seed': 42,
                              'loss_function': "MultiClass",
                              "colsample_bylevel": 0.9759404466998405}


final_model = CatBoostClassifier(**best_params_feat_selection, verbose=False)


final_model.fit(X, y)


predictions = final_model.predict(X_test)
predictions_proba = final_model.predict_proba(X_test)


funcs.metrics_merged(y_test, predictions, predictions_proba)


n_classes = len(np.unique(y_test))
class_labels = ["Control", "MCI", "AD"]
colors = ['blue', 'red', 'green']  # Define colors for the ROC curves

# Create a plot for ROC curves
plt.figure(figsize=(10, 8))

for i in range(n_classes):
    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test == i, predictions_proba[:, i])  # True binary labels for each class
    roc_auc = auc(fpr, tpr)  # Calculate AUC

    # Plot ROC curve
    plt.plot(fpr, tpr, color=colors[i], label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')

# Plotting the diagonal line
plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')  # Diagonal line for random guessing
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves for Multi-class')
plt.legend(loc='lower right')
plt.grid()
plt.show()





data_feat = data[feat]
outcome = data['DIAGNOSIS']


pipeline = Pipeline([("scaler", StandardScaler()), ("pca", PCA(n_components=2)),])


pca_data = pd.DataFrame(
    pipeline.fit_transform(data_feat),
    columns=["PC1", "PC2"],
    index=data_feat.index
)
pca_data['outcome'] = outcome
pca_step = pipeline.steps[1][1]





g = sns.scatterplot(data=pca_data, x="PC1", y="PC2", hue="outcome")


# Add variance explained by the
g.set_xlabel(f"PC1 ({pca_step.explained_variance_ratio_[0]*100:.2f} %)")
g.set_ylabel(f"PC2 ({pca_step.explained_variance_ratio_[1]*100:.2f} %)")

#plt.savefig("PCA_with_loadings.tiff", dpi=300)
plt.show()


pca_MCI = pca_data[pca_data["outcome"] == "MCI"]


gmm = GaussianMixture(n_components=3, covariance_type='tied')
labels = gmm.fit_predict(pca_MCI.iloc[:,0:2])


labels_MCI = np.where(labels == 0, "MCI1", np.where(labels == 1, "MCI2", "MCI3"))


pca_data["cluster"] = pca_data["outcome"]
pca_data.loc[pca_data["outcome"] == "MCI", "cluster"] = labels_MCI


g = sns.scatterplot(data=pca_data, x="PC1", y="PC2", hue="cluster")


# Add variance explained by the
g.set_xlabel(f"PC1 ({pca_step.explained_variance_ratio_[0]*100:.2f} %)")
g.set_ylabel(f"PC2 ({pca_step.explained_variance_ratio_[1]*100:.2f} %)")

#plt.savefig("PCA_with_loadings.png", dpi=200)
plt.show()


grouped_data = pca_data.groupby('cluster')[['PC1', 'PC2']].mean()
Z = linkage(grouped_data, method='ward')

# Create a dendrogram to visualize the clustering
plt.figure(figsize=(10, 7))
dendrogram(Z, labels=grouped_data.index)  # 'outcome' includes AZ, Control, MCI1, MCI2, MCI3
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()


data_feat_cluster = data_feat.copy()
data_feat_cluster["cluster"] = pca_data["cluster"]


data_feat_cluster['cluster'] = data_feat_cluster['cluster'].replace({'MCI1': 'MCI_Middle', 'MCI2':'MCI_AD', 'MCI3': 'MCI_Healthy'})


for var in cont_features_sig:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x='cluster', y=var, data=data_feat_cluster)
    plt.title(f'Boxplot of {var} by MCI1, MCI2, MCI3')
    plt.show()

    # Perform statistical test (ANOVA or Kruskal-Wallis)
    mci1_data = data_feat_cluster[data_feat_cluster['cluster'] == 'MCI_Middle'][var]
    mci2_data = data_feat_cluster[data_feat_cluster['cluster'] == 'MCI_AD'][var]
    mci3_data = data_feat_cluster[data_feat_cluster['cluster'] == 'MCI_Healthy'][var]


    stat, p_value = kruskal(mci1_data, mci2_data, mci3_data)
    print(f"Kruskal-Wallis for {var}: H = {stat:.3f}, p = {p_value:.3f}")
